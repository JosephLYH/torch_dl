{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7aE6Rq3cAEE"
      },
      "source": [
        "# Overview\n",
        "In this toturial, we use guide you step by step to show you how the most basic modules in Tianshou work and how they collaborate with each other to conduct a classic DRL experiment."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1_mLTSEIcY2c"
      },
      "source": [
        "## Run the code\n",
        "Before we get started, we must first install Tianshou's library and gym environment by running the commands below. Here I choose a specific version of Tianshou(0.4.8) which is the latest as of the time writing this toturial. APIs in differet versions may vary a little bit but most are the same. Feel free to use other versions in your own project. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvplhjduVDs6"
      },
      "outputs": [],
      "source": [
        "# !pip install tianshou==0.4.8\n",
        "# !pip install gym"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IcFNmCjYeIIU"
      },
      "source": [
        "Below is a short script that use a certain DRL algorithm (PPO) to solve the classic CartPole-v0\n",
        "problem in gym. Simply run it and **don't worry** if you can't understand the code very well. That is\n",
        "exactly what this tutorial is for.\n",
        "\n",
        "If the script ends normally, you will see the evaluation result printed out before the first\n",
        "epoch is done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxY_ZbGmkr6_",
        "outputId": "b792fc24-f42c-426a-9d83-fe1a4f3f91f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch #1: 50001it [00:11, 4180.03it/s, env_step=50000, len=141, loss=51.011, loss/clip=-0.002, loss/ent=0.569, loss/vf=102.038, n/ep=19, n/st=2000, rew=141.53]                           \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #1: test_reward: 200.000000 ± 0.000000, best_reward: 200.000000 ± 0.000000 in #1\n",
            "{'duration': '13.81s', 'train_time/model': '7.93s', 'test_step': 2090, 'test_episode': 20, 'test_time': '1.85s', 'test_speed': '1131.15 step/s', 'best_reward': 200.0, 'best_result': '200.00 ± 0.00', 'train_step': 50000, 'train_episode': 1020, 'train_time/collector': '4.04s', 'train_speed': '4178.20 step/s'}\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from tianshou.data import Collector, VectorReplayBuffer\n",
        "from tianshou.env import DummyVectorEnv\n",
        "from tianshou.policy import PPOPolicy\n",
        "from tianshou.trainer import onpolicy_trainer\n",
        "from tianshou.utils.net.common import ActorCritic, Net\n",
        "from tianshou.utils.net.discrete import Actor, Critic\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# environments\n",
        "env = gym.make('CartPole-v0')\n",
        "train_envs = DummyVectorEnv([lambda: gym.make('CartPole-v0') for _ in range(20)])\n",
        "test_envs = DummyVectorEnv([lambda: gym.make('CartPole-v0') for _ in range(10)])\n",
        "\n",
        "# model & optimizer\n",
        "net = Net(env.observation_space.shape, hidden_sizes=[64, 64], device=device)\n",
        "actor = Actor(net, env.action_space.n, device=device).to(device)\n",
        "critic = Critic(net, device=device).to(device)\n",
        "actor_critic = ActorCritic(actor, critic)\n",
        "optim = torch.optim.Adam(actor_critic.parameters(), lr=0.0003)\n",
        "\n",
        "# PPO policy\n",
        "dist = torch.distributions.Categorical\n",
        "policy = PPOPolicy(actor, critic, optim, dist, action_space=env.action_space, deterministic_eval=True)\n",
        "        \n",
        "          \n",
        "# collector\n",
        "train_collector = Collector(policy, train_envs, VectorReplayBuffer(20000, len(train_envs)))\n",
        "test_collector = Collector(policy, test_envs)\n",
        "\n",
        "# trainer\n",
        "result = onpolicy_trainer(\n",
        "    policy,\n",
        "    train_collector,\n",
        "    test_collector,\n",
        "    max_epoch=10,\n",
        "    step_per_epoch=50000,\n",
        "    repeat_per_collect=10,\n",
        "    episode_per_test=10,\n",
        "    batch_size=256,\n",
        "    step_per_collect=2000,\n",
        "    stop_fn=lambda mean_reward: mean_reward >= 195,\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9YEQptYvCgx",
        "outputId": "2a9b5b22-be50-4bb7-ae93-af7e65e7442a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final reward: 200.0, length: 200.0\n"
          ]
        }
      ],
      "source": [
        "# Let's watch its performance!\n",
        "policy.eval()\n",
        "result = test_collector.collect(n_episode=1, render=False)\n",
        "print(\"Final reward: {}, length: {}\".format(result[\"rews\"].mean(), result[\"lens\"].mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFYlcPo8fpPU"
      },
      "source": [
        "## Tutorial Introduction\n",
        "\n",
        "A common DRL experiment as is shown above may require many components to work together. The agent, the\n",
        "environment (possibly parallelized ones), the replay buffer and the trainer all work together to complete a\n",
        "training task. \n",
        "\n",
        "<div align=center>\n",
        "<img src=\"https://tianshou.readthedocs.io/en/master/_images/pipeline.png\", width=500>\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV_uOyimj-bk"
      },
      "source": [
        "In Tianshou, all of these main components are factored out as different building blocks, which you\n",
        "can use to create your own algorithm and finish your own experiment.\n",
        "\n",
        "Buiding blocks may include:\n",
        "- Batch\n",
        "- Replay Buffer\n",
        "- Vectorized Environment Wrapper\n",
        "- Policy (the agent and the training algorithm)\n",
        "- Data Collector\n",
        "- Trainer\n",
        "- Logger\n",
        "\n",
        "\n",
        "Check this [webpage](https://tianshou.readthedocs.io/en/master/tutorials/dqn.html) to find jupter-notebook-style tutorials that will guide you through all these\n",
        "modules one by one. You can also read the [documentation](https://tianshou.readthedocs.io/en/master/) of Tianshou for more detailed explanation and\n",
        "advanced usages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0mNKwH9i6Ek"
      },
      "source": [
        "# Further reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3NPSUnAov4L"
      },
      "source": [
        "## What if I am not familar with the PPO algorithm itself?\n",
        "As for the DRL algorithms themselves, we will refer you to the [Spinning up documentation](https://spinningup.openai.com/en/latest/algorithms/ppo.html), where they provide\n",
        "plenty of resources and guides if you want to study the DRL algorithms. In Tianshou's toturials, we will\n",
        "focus on the usages of different modules, but not the algorithms themselves."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
